{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed719884",
   "metadata": {},
   "source": [
    "# 필요 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf0895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:38:55.253185Z",
     "start_time": "2024-04-14T16:38:55.248156Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006c5dd",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6d996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:38:55.299621Z",
     "start_time": "2024-04-14T16:38:55.255186Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/recomm/user_item_interactions.csv')\n",
    "data = data.rename(columns={'TRAVELER_ID': 'user_id', 'POI_ID': 'item_id'})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa513d1f",
   "metadata": {},
   "source": [
    "### 긍정 상호 작용 데이터 설정 (별점 3점이상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523a24bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:38:55.315140Z",
     "start_time": "2024-04-14T16:38:55.302013Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data[data['STARS'] >= 3]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb56777b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:38:55.330361Z",
     "start_time": "2024-04-14T16:38:55.319653Z"
    }
   },
   "outputs": [],
   "source": [
    "data.groupby(['STARS'])['STARS'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722528d6",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb7f8c",
   "metadata": {},
   "source": [
    "## Train & Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349ee15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:38:55.360709Z",
     "start_time": "2024-04-14T16:38:55.333873Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data.values, test_size = 0.2, random_state = 42)\n",
    "train = pd.DataFrame(train, columns = data.columns)\n",
    "test = pd.DataFrame(test, columns = data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46452d12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:38:55.375912Z",
     "start_time": "2024-04-14T16:38:55.363557Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Train size : ', len(train))\n",
    "print('Test size : ', len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48117ce",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b16e65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:38:55.421629Z",
     "start_time": "2024-04-14T16:38:55.379423Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "user_label_encoder = LabelEncoder()\n",
    "item_label_encoder = LabelEncoder()\n",
    "\n",
    "train['user_id_idx'] = user_label_encoder.fit_transform(train['user_id'].values)\n",
    "train['item_id_idx'] = item_label_encoder.fit_transform(train['item_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab12c24e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:38:55.436384Z",
     "start_time": "2024-04-14T16:38:55.424033Z"
    }
   },
   "outputs": [],
   "source": [
    "train_user_ids = train['user_id'].unique()\n",
    "train_item_ids = train['item_id'].unique()\n",
    "print(\"훈련 데이터셋에 있는 유저 수:\", len(train_user_ids), \"훈련 데이터셋에 있는 아이템 수:\", len(train_item_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26c0e73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:38:55.467056Z",
     "start_time": "2024-04-14T16:38:55.438386Z"
    }
   },
   "outputs": [],
   "source": [
    "test = test[(test['user_id'].isin(train_user_ids)) & (test['item_id'].isin(train_item_ids))]\n",
    "test['user_id_idx'] = user_label_encoder.transform(test['user_id'].values)\n",
    "test['item_id_idx'] = item_label_encoder.transform(test['item_id'].values)\n",
    "print(\"테스트 데이터셋 크기:\", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72da0a2",
   "metadata": {},
   "source": [
    "# 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a78f875",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:38:55.482031Z",
     "start_time": "2024-04-14T16:38:55.472077Z"
    }
   },
   "outputs": [],
   "source": [
    "# dok_mtrx를 COO 형식으로 변환하여 희소 텐서로 변환하는 함수\n",
    "def convert_to_sparse_tensor(dok_mtrx):\n",
    "    # dok_mtrx를 COO 형식으로 변환\n",
    "    dok_mtrx_coo = dok_mtrx.tocoo().astype(np.float32)\n",
    "\n",
    "    # COO 형식으로 변환된 행렬에서 값과 인덱스 추출\n",
    "    values = dok_mtrx_coo.data\n",
    "    indices = np.vstack((dok_mtrx_coo.row, dok_mtrx_coo.col))\n",
    "\n",
    "    # 인덱스를 torch.LongTensor로 변환하고 값은 torch.FloatTensor로 변환\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "\n",
    "    # 행렬의 모양 추출\n",
    "    shape = dok_mtrx_coo.shape\n",
    "\n",
    "    # 변환된 값과 인덱스를 사용하여 희소 텐서 생성\n",
    "    dok_mtrx_sparse_tensor = torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "\n",
    "    # 생성된 희소 텐서 반환\n",
    "    return dok_mtrx_sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821fa749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:38:55.497560Z",
     "start_time": "2024-04-14T16:38:55.484542Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "\n",
    "# 사용자 및 항목 간의 관련성 점수를 계산하고 평가 메트릭을 반환하는 함수\n",
    "def get_metrics(W_u, W_i, n_users, n_items, train_data, test_data, K):\n",
    "    # 테스트에 사용되는 사용자 ID 추출\n",
    "    test_user_ids = torch.LongTensor(test_data['user_id_idx'].unique())\n",
    "    \n",
    "    # 사용자 및 항목 간의 관련성 점수 계산\n",
    "    relevance_score = torch.matmul(W_u, torch.transpose(W_i, 0, 1))\n",
    "    \n",
    "    # 훈련 데이터로 희소 행렬 생성\n",
    "    R = sparse.dok_matrix((n_users, n_items), dtype=np.float32)\n",
    "    R[train_data['user_id_idx'], train_data['item_id_idx']] = 1.0\n",
    "    \n",
    "    # 희소 행렬을 PyTorch 희소 텐서로 변환\n",
    "    R_tensor = convert_to_sparse_tensor(R)\n",
    "    \n",
    "    # 희소 텐서를 밀집 텐서로 변환하고 무한대를 0으로 대체\n",
    "    R_tensor_dense = R_tensor.to_dense()\n",
    "    R_tensor_dense = R_tensor_dense * (-np.inf)\n",
    "    R_tensor_dense = torch.nan_to_num(R_tensor_dense, nan=0.0)\n",
    "    \n",
    "    # 관련성 점수에 훈련 데이터를 추가하여 아이템이 이미 상호 작용한 경우 관련성 점수를 음의 무한대로 설정\n",
    "    relevance_score = relevance_score + R_tensor_dense\n",
    "    \n",
    "    # 상위 K개의 인덱스 추출\n",
    "    topk_idx = torch.topk(relevance_score, K).indices\n",
    "    \n",
    "    # 상위 K개의 인덱스를 데이터프레임으로 변환\n",
    "    topk_idx_df = pd.DataFrame(topk_idx.numpy(), columns=['top_idx' + str(x + 1) for x in range(K)])\n",
    "    topk_idx_df['user_id'] = topk_idx_df.index\n",
    "    topk_idx_df['topk_item'] = topk_idx_df[['top_idx' + str(x + 1) for x in range(K)]].values.tolist()\n",
    "    topk_idx_df = topk_idx_df[['user_id', 'topk_item']]\n",
    "    \n",
    "    # 테스트 데이터의 아이템 목록 추출\n",
    "    test_items = test_data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()\n",
    "    \n",
    "    # 테스트 데이터와 상위 K개의 아이템 목록을 결합\n",
    "    metrics_df = pd.merge(test_items, topk_idx_df, how='left', left_on='user_id_idx', right_on='user_id')\n",
    "    \n",
    "    # 테스트 데이터와 상위 K개의 아이템 목록에서 상호 작용하는 아이템 수 계산\n",
    "    metrics_df['interact_items'] = [list(set(a).intersection(b)) for a, b in zip(metrics_df.item_id_idx, metrics_df.topk_item)]\n",
    "    \n",
    "    # 리콜 계산\n",
    "    metrics_df['recall'] = metrics_df.apply(lambda x: len(x['interact_items']) / len(x['item_id_idx']), axis=1)\n",
    "    \n",
    "    # DCG 및 nDCG 계산을 위한 함수 정의\n",
    "    def get_dcg_idcg(item_id_idx, hit_list):\n",
    "        dcg = sum([hit * np.reciprocal(np.log1p(idx + 1)) for idx, hit in enumerate(hit_list)])\n",
    "        idcg = sum([np.reciprocal(np.log1p(idx + 1)) for idx in range(min(len(item_id_idx), len(hit_list)))])\n",
    "        return dcg / idcg\n",
    "    \n",
    "    # 히트 목록 및 nDCG 계산\n",
    "    metrics_df['hit_list'] = metrics_df.apply(lambda x: [1 if i in set(x['item_id_idx']) else 0 for i in x['topk_item']], axis=1)\n",
    "    metrics_df['ndcg'] = metrics_df.apply(lambda x: get_dcg_idcg(x['item_id_idx'], x['hit_list']), axis=1)\n",
    "    \n",
    "    # 평균 리콜 및 nDCG 반환\n",
    "    return metrics_df['recall'].mean(), metrics_df['ndcg'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8cb54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:38:55.512650Z",
     "start_time": "2024-04-14T16:38:55.499582Z"
    }
   },
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, data, n_users, n_items, n_layers, latent_dim):\n",
    "        super(LightGCN, self).__init__()\n",
    "        self.data = data\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.n_layers = n_layers\n",
    "        self.latent_dim = latent_dim\n",
    "        self.init_embedding()\n",
    "        self.norm_adj_mat_sparse_tensor = self.get_norm_adj()\n",
    "    \n",
    "    # 초기 임베딩을 설정하는 메서드\n",
    "    def init_embedding(self):\n",
    "        self.E0 = nn.Embedding(self.n_users + self.n_items, self.latent_dim)\n",
    "        nn.init.xavier_uniform_(self.E0.weight)\n",
    "        self.E0.weight = nn.Parameter(self.E0.weight)\n",
    "    \n",
    "    # 정규화된 인접 행렬을 생성하는 메서드\n",
    "    def get_norm_adj(self):\n",
    "        R = sparse.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "        R[self.data['user_id_idx'], self.data['item_id_idx']] = 1.0\n",
    "        adj_mat = sparse.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "        rowsum = np.array(adj_mat.sum(1))\n",
    "        d_inv = np.power(rowsum + 1e-9, -0.5).flatten()\n",
    "        d_mat_inv = sparse.diags(d_inv)\n",
    "        \n",
    "        # D^(-1/2) _A_ D^(-1/2) 계산\n",
    "        norm_adj_mat = d_mat_inv.dot(adj_mat).dot(d_mat_inv)\n",
    "        norm_adj_mat_coo = norm_adj_mat.tocoo().astype(np.float32)\n",
    "        values = norm_adj_mat_coo.data\n",
    "        indices = np.vstack((norm_adj_mat_coo.row, norm_adj_mat_coo.col))\n",
    "        i = torch.LongTensor(indices)\n",
    "        v = torch.FloatTensor(values)\n",
    "        return torch.sparse.FloatTensor(i, v, torch.Size(norm_adj_mat_coo.shape))\n",
    "    \n",
    "    # 레이어를 통과시키는 메서드\n",
    "    def propagate_through_layers(self):\n",
    "        all_layers_embedding = [self.E0.weight]\n",
    "        E_lyr = self.E0.weight\n",
    "        for layer in range(self.n_layers):\n",
    "            E_lyr = torch.sparse.mm(self.norm_adj_mat_sparse_tensor, E_lyr)\n",
    "            all_layers_embedding.append(E_lyr)\n",
    "        all_layers_embedding = torch.stack(all_layers_embedding)\n",
    "        mean_layer_embedding = torch.mean(all_layers_embedding, axis=0)\n",
    "        return torch.split(mean_layer_embedding, [self.n_users, self.n_items]) + torch.split(self.E0.weight, [self.n_users, self.n_items])\n",
    "    \n",
    "    # 모델의 순전파를 정의하는 메서드\n",
    "    def forward(self, users, pos_items, neg_items):\n",
    "        final_user_emb, final_item_emb, initial_user_emb, initial_item_emb = self.propagate_through_layers()\n",
    "        return (final_user_emb[users], final_item_emb[pos_items], final_item_emb[neg_items], initial_user_emb[users], initial_item_emb[pos_items], initial_item_emb[neg_items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102f1cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:38:55.528195Z",
     "start_time": "2024-04-14T16:38:55.514652Z"
    }
   },
   "outputs": [],
   "source": [
    "def bpr_loss(users, users_emb, pos_emb, neg_emb, user_emb_0, pos_emb_0, neg_emb_0):\n",
    "    # positive 및 negative 샘플에 대한 점수 계산\n",
    "    pos_scores = torch.sum(torch.mul(users_emb, pos_emb), dim=1)\n",
    "    neg_scores = torch.sum(torch.mul(users_emb, neg_emb), dim=1)\n",
    "    \n",
    "    # Bayesian Personalized Ranking (BPR) 손실 계산\n",
    "    loss = -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores)))\n",
    "    \n",
    "    # 정규화 항 추가\n",
    "    if config.reg > 0:\n",
    "        l2_norm = (user_emb_0.norm().pow(2) + pos_emb_0.norm().pow(2) + neg_emb_0.norm().pow(2)) / float(len(users))\n",
    "        reg_loss = config.reg * l2_norm\n",
    "        loss += reg_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf450b",
   "metadata": {},
   "source": [
    "# 모델학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7095a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:38:55.543243Z",
     "start_time": "2024-04-14T16:38:55.530214Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'latent_dim' : 64,\n",
    "    'lr' : 0.001,\n",
    "    'batch_size' : 1024,\n",
    "    'top_k' : 10,\n",
    "    'n_layers' : 3,\n",
    "    'reg' : 1e-4,\n",
    "    'epochs' : 300\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60b0cd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:38:55.558283Z",
     "start_time": "2024-04-14T16:38:55.546244Z"
    }
   },
   "outputs": [],
   "source": [
    "n_users = train['user_id_idx'].nunique()\n",
    "n_items = train['item_id_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acc2379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:38:55.619774Z",
     "start_time": "2024-04-14T16:38:55.560305Z"
    }
   },
   "outputs": [],
   "source": [
    "from box import Box\n",
    "config = Box(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812367f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T17:04:52.745082Z",
     "start_time": "2024-04-08T17:03:09.483733Z"
    }
   },
   "outputs": [],
   "source": [
    "lightGCN = LightGCN(train, n_users, n_items, config.n_layers, config.latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef7705",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:38:55.635243Z",
     "start_time": "2024-04-14T16:38:55.621288Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_loader(data, batch_size, n_users, n_items):\n",
    "    # 사용자별 상호 작용한 항목 목록 추출\n",
    "    interacted_items_df = data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()\n",
    "    \n",
    "    # 부정적 샘플링 함수 정의\n",
    "    def sample_neg(x):\n",
    "        while True:\n",
    "            neg_id = random.randint(0, n_items - 1)\n",
    "            if neg_id not in x:\n",
    "                return neg_id\n",
    "    \n",
    "    # 랜덤하게 사용자 선택\n",
    "    indices = [x for x in range(n_users)]\n",
    "    if n_users < batch_size:\n",
    "        users = [random.choice(indices) for _ in range(batch_size)]\n",
    "    else:\n",
    "        users = random.sample(indices, batch_size)\n",
    "    users.sort()\n",
    "    \n",
    "    # 선택된 사용자와 상호 작용한 항목을 결합하여 데이터프레임 생성\n",
    "    users_df = pd.DataFrame(users, columns=['users'])\n",
    "    interacted_items_df = pd.merge(interacted_items_df, users_df, how='right', left_on='user_id_idx', right_on='users')\n",
    "    \n",
    "    # 양성 샘플 및 부정적 샘플 선택\n",
    "    pos_items = interacted_items_df['item_id_idx'].apply(lambda x: random.choice(x)).values\n",
    "    neg_items = interacted_items_df['item_id_idx'].apply(lambda x: sample_neg(x)).values\n",
    "    \n",
    "    return list(users), list(pos_items), list(neg_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c988d9",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83cdaa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T16:34:25.472066Z",
     "start_time": "2024-04-08T05:45:15.664888Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import math\n",
    "\n",
    "def objective(trial):\n",
    "    # 하이퍼파라미터 설정\n",
    "    config = {\n",
    "        'latent_dim': trial.suggest_categorical('latent_dim', [32, 64, 128]),\n",
    "        'lr': trial.suggest_loguniform('lr', 1e-4, 1e-2),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [256, 512, 1024]),\n",
    "        'num_layers': trial.suggest_int('num_layers', 1, 5),\n",
    "        'reg': trial.suggest_loguniform('reg', 1e-6, 1e-3),\n",
    "        'epochs': 50,  # 튜닝을 위해 에포크 수를 줄임\n",
    "        'top_k': 10\n",
    "    }\n",
    "    \n",
    "    # n_batch 계산\n",
    "    n_batch = math.ceil(len(train) / config['batch_size'])  # train_dataset의 총 개수에 따라 조정 필요\n",
    "    \n",
    "    # 모델 초기화\n",
    "    model = LightGCN(train, n_users, n_items, config['num_layers'], config['latent_dim'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['reg'])\n",
    "    \n",
    "    # 학습 루프\n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        loss_epoch = []\n",
    "        \n",
    "        for batch_idx in range(n_batch):\n",
    "            optimizer.zero_grad()\n",
    "            users, pos_items, neg_items = data_loader(train, config['batch_size'], n_users, n_items)\n",
    "            # 올바른 모델 호출을 위해 코드 수정\n",
    "            users_emb, pos_emb, neg_emb, users_emb_0, pos_emb_0, neg_emb_0 = model.forward(users, pos_items, neg_items)\n",
    "            loss = bpr_loss(users, users_emb, pos_emb, neg_emb, users_emb_0, pos_emb_0, neg_emb_0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_epoch.append(loss.item())\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            final_user_emb, final_item_emb, initial_user_emb, initial_item_emb = model.propagate_through_layers()\n",
    "            test_recall, test_ndcg = get_metrics(final_user_emb, final_item_emb, n_users, n_items, train, test, config['top_k'])\n",
    "        \n",
    "        print(f\"[에폭: {epoch+1}/{config['epochs']}, 손실: {np.mean(loss_epoch):.4f}, 리콜@{config['top_k']}: {test_recall:.4f}, NDCG@{config['top_k']}: {test_ndcg:.4f}]\")\n",
    "    \n",
    "    return test_recall\n",
    "\n",
    "# 하이퍼파라미터 튜닝 수행\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# 최적 하이퍼파라미터 출력\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614e1e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T17:09:35.046479Z",
     "start_time": "2024-04-08T17:09:35.032258Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "optuna 로 최적 모델을 탐색했으나 커널이슈로 기록이 사라짐\n",
    "이슈 발생전 기록해놓은 최적 파라미터값은 아래와 같음\n",
    "\"\"\"\n",
    "# 최적 파라미터로 50 epch를 학습한 모델 저장\n",
    "# Trial 15 finished with value: 0.2492541342615162 and parameters: {'latent_dim': 64, 'lr': 0.009631732140395756, \n",
    "#'batch_size': 256, 'num_layers': 4, 'reg': 1.0794690314138785e-06}. Best is trial 15 with value: 0.2492541342615162."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3287d",
   "metadata": {},
   "source": [
    "## 베스트 하이퍼파라미터 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b10681",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T17:12:02.752478Z",
     "start_time": "2024-04-08T17:09:35.047480Z"
    }
   },
   "outputs": [],
   "source": [
    "# 최적 하이퍼파라미터 설정\n",
    "#best_params = study.best_params\n",
    "\n",
    "\n",
    "# 학습 이력을 바탕으로 확인한 최적의 하이퍼파라미터 설정\n",
    "best_params = {\n",
    "    'latent_dim': 64,  # 최적의 latent dimension 크기\n",
    "    'lr': 0.009631732140395756,  # 최적의 learning rate\n",
    "    'batch_size': 256,  # 최적의 batch size\n",
    "    'num_layers': 4,  # 최적의 layer 수\n",
    "    'reg': 1.0794690314138785e-06,  # 최적의 regularization strength\n",
    "    'epochs': 200  # 최종 학습을 위해 조정한 에포크 수\n",
    "}\n",
    "\n",
    "\n",
    "best_params['epochs'] = 200  # 최종 학습을 위해 에포크 수 조정\n",
    "\n",
    "# 최적 하이퍼파라미터로 모델 학습\n",
    "lightGCN = LightGCN(train, n_users, n_items, best_params['num_layers'], best_params['latent_dim'])\n",
    "optimizer = torch.optim.Adam(lightGCN.parameters(), lr=best_params['lr'], weight_decay=best_params['reg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20b6f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:43:08.665753Z",
     "start_time": "2024-04-08T17:12:52.407641Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "top_k = 10  # top_k 값을 직접 설정\n",
    "\n",
    "loss_list_epoch = []\n",
    "recall_list = []\n",
    "ndcg_list = []\n",
    "n_batch = math.ceil(len(train) / best_params['batch_size'])  # train_dataset의 총 개수에 따라 조정 필요\n",
    "\n",
    "for epoch in range(best_params['epochs']):\n",
    "    lightGCN.train()\n",
    "    final_loss_list = []\n",
    "    \n",
    "    for batch_idx in range(n_batch):\n",
    "        optimizer.zero_grad()\n",
    "        users, pos_items, neg_items = data_loader(train, best_params['batch_size'], n_users, n_items)\n",
    "        users_emb, pos_emb, neg_emb, users_emb_0, pos_emb_0, neg_emb_0 = lightGCN.forward(users, pos_items, neg_items)\n",
    "        final_loss = bpr_loss(users, users_emb, pos_emb, neg_emb, users_emb_0, pos_emb_0, neg_emb_0)\n",
    "        final_loss.backward()\n",
    "        optimizer.step()\n",
    "        final_loss_list.append(final_loss.item())\n",
    "    \n",
    "    lightGCN.eval()\n",
    "    with torch.no_grad():\n",
    "        final_user_emb, final_item_emb, initial_user_emb, initial_item_emb = lightGCN.propagate_through_layers()\n",
    "        test_recall, test_ndcg = get_metrics(final_user_emb, final_item_emb, n_users, n_items, train, test, top_k)  # best_params['top_k'] 대신 직접 설정한 top_k 사용\n",
    "    \n",
    "    loss_list_epoch.append(np.mean(final_loss_list))\n",
    "    recall_list.append(test_recall)\n",
    "    ndcg_list.append(test_ndcg)\n",
    "    \n",
    "    print(f\"[에폭: {epoch+1}/{best_params['epochs']}, 손실: {np.mean(final_loss_list):.4f}, 리콜@{top_k}: {test_recall:.4f}, NDCG@{top_k}: {test_ndcg:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ec0e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:43:13.095765Z",
     "start_time": "2024-04-08T18:43:08.667759Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(lightGCN.state_dict(), '../model/LightGCN_best_model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e17b41",
   "metadata": {},
   "source": [
    "# 학습 결과 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0dc08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T23:18:45.654330Z",
     "start_time": "2024-04-08T23:18:44.918310Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 결과 시각화\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(loss_list_epoch)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(recall_list)\n",
    "plt.title('Test Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(ndcg_list)\n",
    "plt.title('Test NDCG')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('NDCG')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e3ba1",
   "metadata": {},
   "source": [
    "# 추천여행지 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b4d8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:02:07.125335Z",
     "start_time": "2024-04-14T15:02:06.915260Z"
    }
   },
   "outputs": [],
   "source": [
    "travel_log = pd.read_csv('../data/travel_log/travel_log.csv', encoding = 'utf-8')\n",
    "travel_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a67da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:51:01.830616Z",
     "start_time": "2024-04-14T16:48:24.139265Z"
    }
   },
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'latent_dim': 64,  # 최적의 latent dimension 크기\n",
    "    'lr': 0.009631732140395756,  # 최적의 learning rate\n",
    "    'batch_size': 256,  # 최적의 batch size\n",
    "    'num_layers': 4,  # 최적의 layer 수\n",
    "    'reg': 1.0794690314138785e-06,  # 최적의 regularization strength\n",
    "    'epochs': 200  # 최종 학습을 위해 조정한 에포크 수\n",
    "}\n",
    "\n",
    "# 저장된 모델 가중치 불러오기\n",
    "model_weights = torch.load('../model/LightGCN_best_model_weights.pth')\n",
    "\n",
    "# 최적 하이퍼파라미터로 모델 학습\n",
    "\"\"\"\n",
    "위에 LightGCN 클래스 선언 필수\n",
    "\"\"\"\n",
    "lightGCN = LightGCN(train, n_users, n_items, best_params['num_layers'], best_params['latent_dim'])\n",
    "\n",
    "# 불러온 가중치를 모델에 적용\n",
    "lightGCN.load_state_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ebaec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T16:01:47.690326Z",
     "start_time": "2024-04-14T16:01:46.736014Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 사용자 ID 입력받기\n",
    "user_id = input(\"사용자 ID를 입력하세요: \")\n",
    "user_idx = user_label_encoder.transform([user_id])[0]\n",
    "\n",
    "lightGCN.eval()\n",
    "with torch.no_grad():\n",
    "    # 사용자 임베딩 가져오기\n",
    "    final_user_emb, final_item_emb, initial_user_emb, initial_item_emb = lightGCN.propagate_through_layers()\n",
    "    user_embedding = final_user_emb[user_idx]\n",
    "\n",
    "    # 아이템별 유사도 점수 계산\n",
    "    similarity_scores = torch.matmul(user_embedding, final_item_emb.transpose(0, 1))\n",
    "    recommended_item_indices = torch.topk(similarity_scores, top_k).indices\n",
    "\n",
    "# 추천된 아이템의 상세 정보를 포함한 데이터 프레임 생성\n",
    "recommended_items = []\n",
    "\n",
    "# 추천된 아이템들의 상세 정보 추출\n",
    "for item_idx in recommended_item_indices:\n",
    "    item_id = item_label_encoder.inverse_transform([item_idx.item()])[0]\n",
    "    item_info = df[df['POI_ID'] == item_id][['POI_ID', 'POI_NM', 'POI_TYPE']].iloc[0]\n",
    "    # item_info에 'REVISIT_INTENTION', 'RCMDTN_INTENTION', 'STARS' 정보 추가\n",
    "    item_info['REVISIT_INTENTION'] = data[data['item_id'] == item_id]['REVISIT_INTENTION'].iloc[0]\n",
    "    item_info['RCMDTN_INTENTION'] = data[data['item_id'] == item_id]['RCMDTN_INTENTION'].iloc[0]\n",
    "    item_info['STARS'] = data[data['item_id'] == item_id]['STARS'].iloc[0]\n",
    "    recommended_items.append(item_info)\n",
    "\n",
    "# 리스트를 데이터프레임으로 변환\n",
    "recommended_items_df = pd.concat(recommended_items, axis=1).transpose()\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"사용자 {user_id}에 대한 Top-{top_k} 추천 아이템:\")\n",
    "display(recommended_items_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405dd4e",
   "metadata": {},
   "source": [
    "# FastAPI 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc809d36",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class UserIDRequest(BaseModel):\n",
    "    user_id: str\n",
    "\n",
    "@app.post(\"/recommend/\")\n",
    "async def recommend(request: UserIDRequest):\n",
    "    user_id = request.user_id\n",
    "    user_idx = user_label_encoder.transform([user_id])[0]\n",
    "\n",
    "    lightGCN.eval()\n",
    "    with torch.no_grad():\n",
    "        # 사용자 임베딩 가져오기\n",
    "        final_user_emb, final_item_emb, initial_user_emb, initial_item_emb = lightGCN.propagate_through_layers()\n",
    "        user_embedding = final_user_emb[user_idx]\n",
    "\n",
    "        # 아이템별 유사도 점수 계산\n",
    "        similarity_scores = torch.matmul(user_embedding, final_item_emb.transpose(0, 1))\n",
    "        recommended_item_indices = torch.topk(similarity_scores, 5).indices  # top_k를 5로 가정\n",
    "\n",
    "    recommended_items = []\n",
    "\n",
    "    for item_idx in recommended_item_indices:\n",
    "        item_id = item_label_encoder.inverse_transform([item_idx.item()])[0]\n",
    "        item_info = df[df['POI_ID'] == item_id][['POI_ID', 'POI_NM', 'POI_TYPE']].iloc[0].to_dict()\n",
    "        item_info['REVISIT_INTENTION'] = data[data['item_id'] == item_id]['REVISIT_INTENTION'].iloc[0]\n",
    "        item_info['RCMDTN_INTENTION'] = data[data['item_id'] == item_id]['RCMDTN_INTENTION'].iloc[0]\n",
    "        item_info['STARS'] = data[data['item_id'] == item_id]['STARS'].iloc[0]\n",
    "        recommended_items.append(item_info)\n",
    "\n",
    "    return {\"user_id\": user_id, \"recommendations\": recommended_items}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72231af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:08:28.275236Z",
     "start_time": "2024-04-14T15:08:28.170601Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# '카멜리아'가 포함된 행을 필터링\n",
    "sample = travel_log[travel_log['POI_NM'].str.contains('카멜리아') | travel_log['VISIT_AREA_NM'].str.contains('카멜리아')]\n",
    "\n",
    "# 'TRAVELER_ID' 별로 'VISIT_AREA_TYPE_CD'의 빈도 계산\n",
    "frequency = sample.groupby('TRAVELER_ID')['VISIT_AREA_TYPE_CD'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce358f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:08:28.769529Z",
     "start_time": "2024-04-14T15:08:28.753235Z"
    }
   },
   "outputs": [],
   "source": [
    "frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87650923",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:04:08.260357Z",
     "start_time": "2024-04-14T15:04:08.203240Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = travel_log[travel_log['POI_NM'].str.contains('카멜리아') | travel_log['VISIT_AREA_NM'].str.contains('카멜리아')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f7da0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:04:25.317560Z",
     "start_time": "2024-04-14T15:04:25.305469Z"
    }
   },
   "outputs": [],
   "source": [
    "sample['TRAVELER_ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce634fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:43:13.128809Z",
     "start_time": "2024-04-08T18:43:13.128809Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 가중치 저장\n",
    "torch.save(lightGCN.state_dict(), '../model/LightGCN_best_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc112b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T23:23:54.765376Z",
     "start_time": "2024-04-08T23:23:54.543848Z"
    }
   },
   "outputs": [],
   "source": [
    "# 관광 메타 데이터 \n",
    "df = pd.read_csv('../data/travel_meta/travel_meta.csv', encoding = 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.448px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
