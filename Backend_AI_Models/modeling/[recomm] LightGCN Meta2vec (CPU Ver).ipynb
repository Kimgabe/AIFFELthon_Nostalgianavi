{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed719884",
   "metadata": {},
   "source": [
    "# 필요 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf0895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:23:53.362278Z",
     "start_time": "2024-04-19T07:23:52.673806Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006c5dd",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6d996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:23:53.407711Z",
     "start_time": "2024-04-19T07:23:53.364281Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/recomm/user_item_interactions.csv')\n",
    "data = data.rename(columns={'TRAVELER_ID': 'user_id', 'POI_ID': 'item_id'})\n",
    "display(data.head())\n",
    "\n",
    "display(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac2845",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:23:53.422715Z",
     "start_time": "2024-04-19T07:23:53.409712Z"
    }
   },
   "outputs": [],
   "source": [
    "data_user_ids = data['user_id'].unique()\n",
    "data_item_ids = data['item_id'].unique()\n",
    "print(\"훈련 데이터셋에 있는 유저 수:\", len(data_user_ids), \"훈련 데이터셋에 있는 아이템 수:\", len(data_item_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df10a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:23:55.374268Z",
     "start_time": "2024-04-19T07:23:53.425712Z"
    }
   },
   "outputs": [],
   "source": [
    "# POIMeta2vec 임베딩 벡터 로드\n",
    "poimeta2vec_df = pd.read_csv('../model/poimeta2vec_df.csv', index_col=0, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722528d6",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb7f8c",
   "metadata": {},
   "source": [
    "## Train & Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349ee15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:23:55.824266Z",
     "start_time": "2024-04-19T07:23:55.375301Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data.values, test_size = 0.2, random_state = 42)\n",
    "train = pd.DataFrame(train, columns = data.columns)\n",
    "test = pd.DataFrame(test, columns = data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46452d12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:23:55.839444Z",
     "start_time": "2024-04-19T07:23:55.825266Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Train size : ', len(train))\n",
    "print('Test size : ', len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48117ce",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b16e65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:23:55.869420Z",
     "start_time": "2024-04-19T07:23:55.840413Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "user_label_encoder = LabelEncoder()\n",
    "item_label_encoder = LabelEncoder()\n",
    "\n",
    "train['user_id_idx'] = user_label_encoder.fit_transform(train['user_id'].values)\n",
    "train['item_id_idx'] = item_label_encoder.fit_transform(train['item_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab12c24e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:23:55.899413Z",
     "start_time": "2024-04-19T07:23:55.872411Z"
    }
   },
   "outputs": [],
   "source": [
    "train_user_ids = train['user_id'].unique()\n",
    "train_item_ids = train['item_id'].unique()\n",
    "print(\"훈련 데이터셋에 있는 유저 수:\", len(train_user_ids), \"훈련 데이터셋에 있는 아이템 수:\", len(train_item_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c20ec8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:23:55.914414Z",
     "start_time": "2024-04-19T07:23:55.902411Z"
    }
   },
   "outputs": [],
   "source": [
    "test_user_ids = test['user_id'].unique()\n",
    "test_item_ids = test['item_id'].unique()\n",
    "print(\"테스트 데이터셋에 있는 유저 수:\", len(test_user_ids), \"테스트 데이터셋에 있는 아이템 수:\", len(test_item_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26c0e73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:23:55.944411Z",
     "start_time": "2024-04-19T07:23:55.916413Z"
    }
   },
   "outputs": [],
   "source": [
    "test = test[(test['user_id'].isin(train_user_ids)) & (test['item_id'].isin(train_item_ids))]\n",
    "test['user_id_idx'] = user_label_encoder.transform(test['user_id'].values)\n",
    "test['item_id_idx'] = item_label_encoder.transform(test['item_id'].values)\n",
    "print(\"테스트 데이터셋 크기:\", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72da0a2",
   "metadata": {},
   "source": [
    "# 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a78f875",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:23:55.959412Z",
     "start_time": "2024-04-19T07:23:55.949413Z"
    }
   },
   "outputs": [],
   "source": [
    "# dok_mtrx를 COO 형식으로 변환하여 희소 텐서로 변환하는 함수\n",
    "def convert_to_sparse_tensor(dok_mtrx):\n",
    "    # dok_mtrx를 COO 형식으로 변환\n",
    "    dok_mtrx_coo = dok_mtrx.tocoo().astype(np.float32)\n",
    "\n",
    "    # COO 형식으로 변환된 행렬에서 값과 인덱스 추출\n",
    "    values = dok_mtrx_coo.data\n",
    "    indices = np.vstack((dok_mtrx_coo.row, dok_mtrx_coo.col))\n",
    "\n",
    "    # 인덱스를 torch.LongTensor로 변환하고 값은 torch.FloatTensor로 변환\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "\n",
    "    # 행렬의 모양 추출\n",
    "    shape = dok_mtrx_coo.shape\n",
    "\n",
    "    # 변환된 값과 인덱스를 사용하여 희소 텐서 생성\n",
    "    dok_mtrx_sparse_tensor = torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "\n",
    "    # 생성된 희소 텐서 반환\n",
    "    return dok_mtrx_sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821fa749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:23:55.974411Z",
     "start_time": "2024-04-19T07:23:55.962413Z"
    }
   },
   "outputs": [],
   "source": [
    "# cpu연산용\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "def get_metrics(W_u, W_i, n_users, n_items, train_data, test_data, K):\n",
    "    test_user_ids = torch.LongTensor(test_data['user_id_idx'].unique())\n",
    "    relevance_score = torch.matmul(W_u, torch.transpose(W_i, 0, 1))\n",
    "    R = sparse.dok_matrix((n_users, n_items), dtype=np.float32)\n",
    "    R[train_data['user_id_idx'], train_data['item_id_idx']] = 1.0\n",
    "    R_tensor = convert_to_sparse_tensor(R)\n",
    "    R_tensor_dense = R_tensor.to_dense()\n",
    "    R_tensor_dense = R_tensor_dense * (-np.inf)\n",
    "    R_tensor_dense = torch.where(torch.isnan(R_tensor_dense), torch.full_like(R_tensor_dense, 0), R_tensor_dense)\n",
    "    relevance_score = relevance_score + R_tensor_dense\n",
    "    topk_idx = torch.topk(relevance_score, K).indices\n",
    "    topk_idx_df = pd.DataFrame(topk_idx.numpy(), columns=['top_idx' + str(x + 1) for x in range(K)])\n",
    "    topk_idx_df['user_id'] = topk_idx_df.index\n",
    "    topk_idx_df['topk_item'] = topk_idx_df[['top_idx' + str(x + 1) for x in range(K)]].values.tolist()\n",
    "    topk_idx_df = topk_idx_df[['user_id', 'topk_item']]\n",
    "    test_items = test_data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()\n",
    "    metrics_df = pd.merge(test_items, topk_idx_df, how='left', left_on='user_id_idx', right_on='user_id')\n",
    "    metrics_df['interact_items'] = [list(set(a).intersection(b)) for a, b in\n",
    "                                    zip(metrics_df.item_id_idx, metrics_df.topk_item)]\n",
    "    metrics_df['recall'] = metrics_df.apply(lambda x: len(x['interact_items']) / len(x['item_id_idx']), axis=1)\n",
    "\n",
    "    def get_dcg_idcg(item_id_idx, hit_list):\n",
    "        dcg = sum([hit * np.reciprocal(np.log1p(idx + 1)) for idx, hit in enumerate(hit_list)])\n",
    "        idcg = sum([np.reciprocal(np.log1p(idx + 1)) for idx in range(min(len(item_id_idx), len(hit_list)))])\n",
    "        return dcg / idcg\n",
    "\n",
    "    metrics_df['hit_list'] = metrics_df.apply(lambda x: [1 if i in set(x['item_id_idx']) else 0 for i in x['topk_item']],\n",
    "                                              axis=1)\n",
    "    metrics_df['ndcg'] = metrics_df.apply(lambda x: get_dcg_idcg(x['item_id_idx'], x['hit_list']), axis=1)\n",
    "    return metrics_df['recall'].mean(), metrics_df['ndcg'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8cb54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:23:55.989437Z",
     "start_time": "2024-04-19T07:23:55.975413Z"
    }
   },
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, data, n_users, n_items, n_layers, latent_dim, poimeta2vec_df):\n",
    "        super(LightGCN, self).__init__()\n",
    "        self.data = data\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.n_layers = n_layers\n",
    "        self.latent_dim = latent_dim\n",
    "        self.poimeta2vec_df = poimeta2vec_df\n",
    "        self.init_embedding()\n",
    "        self.norm_adj_mat_sparse_tensor = self.get_norm_adj()\n",
    "\n",
    "    def init_embedding(self):\n",
    "        self.E0 = nn.Embedding(self.n_users + self.n_items, self.latent_dim)\n",
    "        nn.init.xavier_uniform_(self.E0.weight)\n",
    "        self.E0.weight = nn.Parameter(self.E0.weight)\n",
    "\n",
    "    def get_norm_adj(self):\n",
    "        R = sparse.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "        R[self.data['user_id_idx'], self.data['item_id_idx']] = 1.0\n",
    "        adj_mat = sparse.dok_matrix(\n",
    "            (self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "        rowsum = np.array(adj_mat.sum(1))\n",
    "        d_inv = np.power(rowsum + 1e-9, -0.5).flatten()\n",
    "        d_mat_inv = sparse.diags(d_inv)\n",
    "        norm_adj_mat = d_mat_inv.dot(adj_mat).dot(d_mat_inv)\n",
    "        norm_adj_mat_coo = norm_adj_mat.tocoo().astype(np.float32)\n",
    "        values = norm_adj_mat_coo.data\n",
    "        indices = np.vstack((norm_adj_mat_coo.row, norm_adj_mat_coo.col))\n",
    "        i = torch.LongTensor(indices)\n",
    "        v = torch.FloatTensor(values)\n",
    "        return torch.sparse.FloatTensor(i, v, torch.Size(norm_adj_mat_coo.shape))\n",
    "\n",
    "    def propagate_through_layers(self):\n",
    "        all_layers_embedding = [self.E0.weight]\n",
    "        E_lyr = self.E0.weight\n",
    "        for layer in range(self.n_layers):\n",
    "            E_lyr = torch.sparse.mm(self.norm_adj_mat_sparse_tensor, E_lyr)\n",
    "            all_layers_embedding.append(E_lyr)\n",
    "        all_layers_embedding = torch.stack(all_layers_embedding)\n",
    "        mean_layer_embedding = torch.mean(all_layers_embedding, axis=0)\n",
    "        return torch.split(mean_layer_embedding, [self.n_users, self.n_items]) + torch.split(self.E0.weight, [self.n_users, self.n_items])\n",
    "\n",
    "    def forward(self, users, pos_items, neg_items, poimeta2vec_tensor):\n",
    "        final_user_emb, final_item_emb, initial_user_emb, initial_item_emb = self.propagate_through_layers()\n",
    "        pos_poimeta2vec_emb = poimeta2vec_tensor[pos_items]\n",
    "        neg_poimeta2vec_emb = poimeta2vec_tensor[neg_items]\n",
    "        final_item_emb_pos = final_item_emb[pos_items] + pos_poimeta2vec_emb\n",
    "        final_item_emb_neg = final_item_emb[neg_items] + neg_poimeta2vec_emb\n",
    "        initial_item_emb_pos = initial_item_emb[pos_items] + pos_poimeta2vec_emb\n",
    "        initial_item_emb_neg = initial_item_emb[neg_items] + neg_poimeta2vec_emb\n",
    "        return (final_user_emb[users], final_item_emb_pos, final_item_emb_neg, initial_user_emb[users], initial_item_emb_pos, initial_item_emb_neg)\n",
    "\n",
    "    def get_poimeta2vec_emb(self, item_ids):\n",
    "        poimeta2vec_emb = []\n",
    "        for item_id in item_ids:\n",
    "            item_poimeta2vec = self.poimeta2vec_df.loc[self.poimeta2vec_df['POI_ID'] == item_id]\n",
    "            if len(item_poimeta2vec) > 0:\n",
    "                poimeta2vec_emb.append(item_poimeta2vec.values[0][1:])\n",
    "            else:\n",
    "                # 해당 item_id에 대한 POIMeta2vec 임베딩이 없는 경우 랜덤 값으로 초기화\n",
    "                poimeta2vec_emb.append(np.random.normal(size=self.latent_dim))\n",
    "        return torch.tensor(poimeta2vec_emb, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca8f78b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:23:56.004412Z",
     "start_time": "2024-04-19T07:23:55.994411Z"
    }
   },
   "outputs": [],
   "source": [
    "# cpu 연산용\n",
    "def bpr_loss(users, users_emb, pos_emb, neg_emb, user_emb_0, pos_emb_0, neg_emb_0, config):\n",
    "    pos_scores = torch.sum(torch.mul(users_emb, pos_emb), dim=1)\n",
    "    neg_scores = torch.sum(torch.mul(users_emb, neg_emb), dim=1)\n",
    "    loss = -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores)))\n",
    "    if config.reg > 0:\n",
    "        l2_norm = (user_emb_0.norm().pow(2) + pos_emb_0.norm().pow(2) + neg_emb_0.norm().pow(2)) / float(len(users))\n",
    "        reg_loss = config.reg * l2_norm\n",
    "        loss += reg_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf450b",
   "metadata": {},
   "source": [
    "# 모델학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60b0cd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:23:56.019412Z",
     "start_time": "2024-04-19T07:23:56.007411Z"
    }
   },
   "outputs": [],
   "source": [
    "n_users = train['user_id_idx'].nunique()\n",
    "n_items = train['item_id_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef7705",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:23:56.079412Z",
     "start_time": "2024-04-19T07:23:56.021411Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# cpu연산용\n",
    "def data_loader(data, batch_size, n_users, n_items, poimeta2vec_df, latent_dim):\n",
    "    interacted_items_df = data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()\n",
    "\n",
    "    def sample_neg(x):\n",
    "        while True:\n",
    "            neg_id = random.randint(0, n_items - 1)\n",
    "            if neg_id not in x:\n",
    "                return neg_id\n",
    "\n",
    "    indices = [x for x in range(n_users)]\n",
    "    if n_users < batch_size:\n",
    "        users = [random.choice(indices) for _ in range(batch_size)]\n",
    "    else:\n",
    "        users = random.sample(indices, batch_size)\n",
    "    users.sort()\n",
    "    users_df = pd.DataFrame(users, columns=['users'])\n",
    "    interacted_items_df = pd.merge(interacted_items_df, users_df, how='right', left_on='user_id_idx',\n",
    "                                   right_on='users')\n",
    "    \n",
    "    pos_items = interacted_items_df['item_id_idx'].apply(lambda x: random.choice(x) if isinstance(x, list) else -1).values\n",
    "    neg_items = interacted_items_df['item_id_idx'].apply(lambda x: sample_neg(x) if isinstance(x, list) else -1).values\n",
    "    \n",
    "    # NaN 값을 -1로 대체\n",
    "    pos_items = np.where(np.isnan(pos_items), -1, pos_items).astype(int)\n",
    "    neg_items = np.where(np.isnan(neg_items), -1, neg_items).astype(int)\n",
    "    \n",
    "    poimeta2vec_emb_dim = poimeta2vec_df.shape[1] - 1\n",
    "    if poimeta2vec_emb_dim != latent_dim:\n",
    "        poimeta2vec_emb = poimeta2vec_df.drop('POI_ID', axis=1).values\n",
    "        if poimeta2vec_emb_dim > latent_dim:\n",
    "            pca = PCA(n_components=latent_dim)\n",
    "            poimeta2vec_emb = pca.fit_transform(poimeta2vec_emb)\n",
    "        else:\n",
    "            poimeta2vec_emb = np.hstack((poimeta2vec_emb, np.random.normal(size=(poimeta2vec_emb.shape[0], latent_dim - poimeta2vec_emb_dim))))\n",
    "        poimeta2vec_df = pd.concat([poimeta2vec_df['POI_ID'], pd.DataFrame(poimeta2vec_emb)], axis=1)\n",
    "    poimeta2vec_tensor = torch.tensor(poimeta2vec_df.drop('POI_ID', axis=1).values, dtype=torch.float32)\n",
    "    return list(users), list(pos_items), list(neg_items), poimeta2vec_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c988d9",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83cdaa2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-16T16:46:09.404Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import math\n",
    "from box import Box\n",
    "\n",
    "# 목적 함수: Optuna 실험을 위한\n",
    "def objective(trial, poimeta2vec_df, train, test, n_users, n_items):\n",
    "    # 하이퍼파라미터 설정\n",
    "    config = {\n",
    "        'latent_dim': trial.suggest_categorical('latent_dim', [32, 64, 128]),  # 임베딩 차원\n",
    "        'lr': trial.suggest_loguniform('lr', 1e-4, 1e-2),  # 학습률\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256]),  # 배치 크기\n",
    "        'num_layers': trial.suggest_int('num_layers', 1, 3),  # 그래프 컨볼루션 층의 수\n",
    "        'reg': trial.suggest_loguniform('reg', 1e-6, 1e-3),  # 정규화 강도\n",
    "        'epochs': 20,  # 에폭 수\n",
    "        'top_k': 10,  # 상위 K개의 추천\n",
    "        'patience': 5  # 조기 종료 기준\n",
    "    }\n",
    "    config = Box(config)  # 설정을 쉽게 접근하기 위해 Box 객체 사용\n",
    "    # 학습 데이터 총 개수를 배치 크기로 나누어 총 배치 수 계산\n",
    "    n_batch = math.ceil(len(train) / config['batch_size'])\n",
    "\n",
    "    # LightGCN 모델 초기화\n",
    "    model = LightGCN(train, n_users, n_items, config['num_layers'], config['latent_dim'], poimeta2vec_df)\n",
    "\n",
    "    # 옵티마이저 설정: Adam 알고리즘 사용\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['reg'])\n",
    "\n",
    "    # 조기 종료를 위한 카운터와 최고 테스트 손실값 초기화\n",
    "    early_stop_counter = 0\n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    # 설정된 에폭 수만큼 반복\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()  # 모델을 학습 모드로 설정\n",
    "        train_loss_epoch = []  # 에폭별 학습 손실을 저장할 리스트\n",
    "\n",
    "        # 각 배치에 대해 반복\n",
    "        for batch_idx in range(n_batch):\n",
    "            optimizer.zero_grad()  # 그라디언트 초기화\n",
    "\n",
    "            # 데이터 로더를 통해 현재 배치의 사용자, 긍정 아이템, 부정 아이템, 메타데이터 임베딩 텐서 가져오기\n",
    "            users, pos_items, neg_items, poimeta2vec_tensor = data_loader(train, config.batch_size, n_users, n_items, poimeta2vec_df, config.latent_dim)\n",
    "\n",
    "            # 모델의 포워드 패스 실행\n",
    "            users_emb, pos_emb, neg_emb, users_emb_0, pos_emb_0, neg_emb_0 = model(users, pos_items, neg_items, poimeta2vec_tensor)\n",
    "\n",
    "            # BPR 손실 계산\n",
    "            train_loss = bpr_loss(users, users_emb, pos_emb, neg_emb, users_emb_0, pos_emb_0, neg_emb_0, config)\n",
    "            train_loss.backward()  # 손실에 대해 역전파 수행\n",
    "            optimizer.step()  # 옵티마이저를 통해 파라미터 업데이트\n",
    "            train_loss_epoch.append(train_loss.item())  # 현재 배치의 손실을 저장\n",
    "\n",
    "        model.eval()  # 모델을 평가 모드로 설정\n",
    "        with torch.no_grad():  # 그라디언트 계산을 비활성화하여 메모리 사용 최소화\n",
    "            # 모델을 통해 사용자와 아이템의 최종 임베딩을 얻음\n",
    "            final_user_emb, final_item_emb, initial_user_emb, initial_item_emb = model.propagate_through_layers()\n",
    "\n",
    "            # 테스트 데이터에 대한 메트릭 계산\n",
    "            test_recall, test_ndcg = get_metrics(final_user_emb, final_item_emb, n_users, n_items, train, test, config['top_k'])\n",
    "\n",
    "            # 테스트 데이터 로딩\n",
    "            test_users, test_pos_items, test_neg_items, test_poimeta2vec_tensor = data_loader(test, config.batch_size, n_users, n_items, poimeta2vec_df, config.latent_dim)\n",
    "\n",
    "            # 테스트 데이터에 대해 모델 포워드 패스 실행\n",
    "            test_users_emb, test_pos_emb, test_neg_emb, test_users_emb_0, test_pos_emb_0, test_neg_emb_0 = model(test_users, test_pos_items, test_neg_items, test_poimeta2vec_tensor)\n",
    "\n",
    "            # 테스트 손실 계산\n",
    "            test_loss = bpr_loss(test_users, test_users_emb, test_pos_emb, test_neg_emb, test_users_emb_0, test_pos_emb_0, test_neg_emb_0, config)\n",
    "\n",
    "        # 에폭별 결과 출력\n",
    "        print(f\"Epoch {epoch+1}/{config['epochs']}, Train Loss: {np.mean(train_loss_epoch):.4f}, Test Loss: {test_loss:.4f}, Recall@{config['top_k']}: {test_recall:.4f}, NDCG@{config['top_k']}: {test_ndcg:.4f}\")\n",
    "\n",
    "        # 조기 종료 로직\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            early_stop_counter = 0  # 최고 손실이 개선되면 카운터를 리셋\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= config.patience:  # 조기 종료 기준 도달 시\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break  # 학습 중단\n",
    "\n",
    "    return best_test_loss\n",
    "\n",
    "# 하이퍼파라미터 튜닝 수행\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, poimeta2vec_df, train, test, n_users, n_items), n_trials=30)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best test loss:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614e1e9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-14T02:28:39.444Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "optuna 로 최적 모델을 탐색했으나 커널이슈로 기록이 사라짐\n",
    "이슈 발생전 기록해놓은 최적 파라미터값은 아래와 같음\n",
    "\"\"\"\n",
    "# 최적 파라미터로 50 epch를 학습한 모델 저장\n",
    "# Trial 15 finished with value: 0.2492541342615162 and parameters: {'latent_dim': 64, 'lr': 0.009631732140395756, \n",
    "#'batch_size': 256, 'num_layers': 4, 'reg': 1.0794690314138785e-06}. Best is trial 15 with value: 0.2492541342615162."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3287d",
   "metadata": {},
   "source": [
    "## 베스트 하이퍼파라미터 학습(w. 앙상블)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb80895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:23:56.094412Z",
     "start_time": "2024-04-19T07:23:56.081415Z"
    }
   },
   "outputs": [],
   "source": [
    "# 앙상블용\n",
    "def bpr_loss(users, users_emb, pos_emb, neg_emb, user_emb_0, pos_emb_0, neg_emb_0, params):\n",
    "    pos_scores = torch.sum(torch.mul(users_emb, pos_emb), dim=1)\n",
    "    neg_scores = torch.sum(torch.mul(users_emb, neg_emb), dim=1)\n",
    "    loss = -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores)))\n",
    "    if params['reg'] > 0:\n",
    "        l2_norm = (user_emb_0.norm().pow(2) + pos_emb_0.norm().pow(2) + neg_emb_0.norm().pow(2)) / float(len(users))\n",
    "        reg_loss = params['reg'] * l2_norm\n",
    "        loss += reg_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cfa7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T11:59:20.592814Z",
     "start_time": "2024-04-19T07:23:56.098414Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 최적 하이퍼파라미터 설정\n",
    "best_params_list = [\n",
    "    {'latent_dim': 128, 'lr': 0.0012643544834896815, 'batch_size': 64, 'num_layers': 3, 'reg': 3.3976282399703575e-06},  # Trial 13\n",
    "    {'latent_dim': 128, 'lr': 0.003323105085677813, 'batch_size': 64, 'num_layers': 3, 'reg': 2.9697461259314475e-06},   # Trial 18\n",
    "    {'latent_dim': 128, 'lr': 0.00023329374686282962, 'batch_size': 64, 'num_layers': 2, 'reg': 7.551153211170191e-06},  # Trial 6\n",
    "    {'latent_dim': 128, 'lr': 0.0028407241056413366, 'batch_size': 128, 'num_layers': 3, 'reg': 3.531882181203301e-06}   # Trial 15\n",
    "]\n",
    "\n",
    "# K-Fold 교차 검증 설정\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# 앙상블에 사용될 모델들을 저장할 리스트\n",
    "models = []\n",
    "\n",
    "# K-Fold 교차 검증 수행\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train)):\n",
    "    print(f\"Fold {fold+1}/{n_splits}\")\n",
    "    \n",
    "    train_fold, val_fold = train.iloc[train_idx], train.iloc[val_idx]\n",
    "    \n",
    "    for best_params in best_params_list:\n",
    "        best_params['epochs'] = 200  # 최종 학습을 위해 에포크 수 조정\n",
    "        best_params['patience'] = 10  # 조기 종료를 위한 patience 값 설정\n",
    "        \n",
    "        # poimeta2vec_df를 DataFrame 형태로 전달\n",
    "        lightGCN = LightGCN(train_fold, n_users, n_items, best_params['num_layers'], best_params['latent_dim'], poimeta2vec_df)\n",
    "        optimizer = torch.optim.Adam(lightGCN.parameters(), lr=best_params['lr'], weight_decay=best_params['reg'])\n",
    "        \n",
    "        top_k = 10  # top_k 값을 직접 설정\n",
    "        \n",
    "        loss_list_epoch = []\n",
    "        recall_list = []\n",
    "        ndcg_list = []\n",
    "        \n",
    "        n_batch = math.ceil(len(train_fold) / best_params['batch_size'])  # train_dataset의 총 개수에 따라 조정 필요\n",
    "        \n",
    "        early_stop_counter = 0\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(best_params['epochs']):\n",
    "            lightGCN.train()\n",
    "            final_loss_list = []\n",
    "            \n",
    "            for batch_idx in range(n_batch):\n",
    "                optimizer.zero_grad()\n",
    "                users, pos_items, neg_items, poimeta2vec_tensor = data_loader(train_fold, best_params['batch_size'], n_users, n_items, poimeta2vec_df, best_params['latent_dim'])\n",
    "                users_emb, pos_emb, neg_emb, users_emb_0, pos_emb_0, neg_emb_0 = lightGCN(users, pos_items, neg_items, poimeta2vec_tensor)\n",
    "                final_loss = bpr_loss(users, users_emb, pos_emb, neg_emb, users_emb_0, pos_emb_0, neg_emb_0, best_params)\n",
    "                final_loss.backward()\n",
    "                optimizer.step()\n",
    "                final_loss_list.append(final_loss.item())\n",
    "            \n",
    "            lightGCN.eval()\n",
    "            with torch.no_grad():\n",
    "                final_user_emb, final_item_emb, initial_user_emb, initial_item_emb = lightGCN.propagate_through_layers()\n",
    "                val_users, val_pos_items, val_neg_items, val_poimeta2vec_tensor = data_loader(val_fold, best_params['batch_size'], n_users, n_items, poimeta2vec_df, best_params['latent_dim'])\n",
    "                val_users_emb, val_pos_emb, val_neg_emb, val_users_emb_0, val_pos_emb_0, val_neg_emb_0 = lightGCN(val_users, val_pos_items, val_neg_items, val_poimeta2vec_tensor)\n",
    "                val_loss = bpr_loss(val_users, val_users_emb, val_pos_emb, val_neg_emb, val_users_emb_0, val_pos_emb_0, val_neg_emb_0, best_params)\n",
    "                val_recall, val_ndcg = get_metrics(final_user_emb, final_item_emb, n_users, n_items, train_fold, val_fold, top_k)\n",
    "            \n",
    "            loss_list_epoch.append(np.mean(final_loss_list))\n",
    "            recall_list.append(val_recall)\n",
    "            ndcg_list.append(val_ndcg)\n",
    "            \n",
    "            print(f\"[에폭: {epoch+1}/{best_params['epochs']}, 손실: {np.mean(final_loss_list):.4f}, 검증 손실: {val_loss:.4f}, 리콜@{top_k}: {val_recall:.4f}, NDCG@{top_k}: {val_ndcg:.4f}]\")\n",
    "            \n",
    "            # 조기 종료 로직\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "            \n",
    "            if early_stop_counter >= best_params['patience']:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        models.append(lightGCN)\n",
    "\n",
    "# 앙상블 평가\n",
    "ensemble_recall, ensemble_ndcg = ensemble_evaluate(models, test, top_k)\n",
    "print(f\"앙상블 결과 - 리콜@{top_k}: {ensemble_recall:.4f}, NDCG@{top_k}: {ensemble_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2556005",
   "metadata": {},
   "source": [
    "## 최종모델 학습\n",
    "\n",
    "- 앙상블 없이 Best Parameter만으로 학습하려는 경우 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b10681",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T17:12:02.752478Z",
     "start_time": "2024-04-08T17:09:35.047480Z"
    }
   },
   "outputs": [],
   "source": [
    "# 최적 하이퍼파라미터 설정\n",
    "#best_params = study.best_params\n",
    "\n",
    "\n",
    "# 학습 이력을 바탕으로 확인한 최적의 하이퍼파라미터 설정\n",
    "best_params = {\n",
    "    'latent_dim': 64,  # 최적의 latent dimension 크기\n",
    "    'lr': 0.009631732140395756,  # 최적의 learning rate\n",
    "    'batch_size': 256,  # 최적의 batch size\n",
    "    'num_layers': 4,  # 최적의 layer 수\n",
    "    'reg': 1.0794690314138785e-06,  # 최적의 regularization strength\n",
    "    'epochs': 200  # 최종 학습을 위해 조정한 에포크 수\n",
    "}\n",
    "\n",
    "\n",
    "best_params['epochs'] = 200  # 최종 학습을 위해 에포크 수 조정\n",
    "\n",
    "# 최적 하이퍼파라미터로 모델 학습\n",
    "lightGCN = LightGCN(train, n_users, n_items, best_params['num_layers'], best_params['latent_dim'], poimeta2vec_df)\n",
    "optimizer = torch.optim.Adam(lightGCN.parameters(), lr=best_params['lr'], weight_decay=best_params['reg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20b6f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:43:08.665753Z",
     "start_time": "2024-04-08T17:12:52.407641Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "top_k = 10  # top_k 값을 직접 설정\n",
    "\n",
    "loss_list_epoch = []\n",
    "recall_list = []\n",
    "ndcg_list = []\n",
    "n_batch = math.ceil(len(train) / best_params['batch_size'])  # train_dataset의 총 개수에 따라 조정 필요\n",
    "\n",
    "for epoch in range(best_params['epochs']):\n",
    "    lightGCN.train()\n",
    "    final_loss_list = []\n",
    "    for batch_idx in range(n_batch):\n",
    "        optimizer.zero_grad()\n",
    "        users, pos_items, neg_items, poimeta2vec_df = data_loader(train, best_params['batch_size'], n_users, n_items, poimeta2vec_df, best_params['latent_dim'])\n",
    "        users_emb, pos_emb, neg_emb, users_emb_0, pos_emb_0, neg_emb_0 = lightGCN.forward(users, pos_items, neg_items)\n",
    "        final_loss = bpr_loss(users, users_emb, pos_emb, neg_emb, users_emb_0, pos_emb_0, neg_emb_0)\n",
    "        final_loss.backward()\n",
    "        optimizer.step()\n",
    "        final_loss_list.append(final_loss.item())\n",
    "    \n",
    "    lightGCN.eval()\n",
    "    with torch.no_grad():\n",
    "        final_user_emb, final_item_emb, initial_user_emb, initial_item_emb = lightGCN.propagate_through_layers()\n",
    "        test_recall, test_ndcg = get_metrics(final_user_emb, final_item_emb, n_users, n_items, train, test, top_k)  # best_params['top_k'] 대신 직접 설정한 top_k 사용\n",
    "    \n",
    "    loss_list_epoch.append(np.mean(final_loss_list))\n",
    "    recall_list.append(test_recall)\n",
    "    ndcg_list.append(test_ndcg)\n",
    "    \n",
    "    print(f\"[에폭: {epoch+1}/{best_params['epochs']}, 손실: {np.mean(final_loss_list):.4f}, 리콜@{top_k}: {test_recall:.4f}, NDCG@{top_k}: {test_ndcg:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ec0e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:43:13.095765Z",
     "start_time": "2024-04-08T18:43:08.667759Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(lightGCN.state_dict(), '../model/LightGCN_best_model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e17b41",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# 학습 결과 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0dc08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T23:18:45.654330Z",
     "start_time": "2024-04-08T23:18:44.918310Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 결과 시각화\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(loss_list_epoch)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(recall_list)\n",
    "plt.title('Test Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(ndcg_list)\n",
    "plt.title('Test NDCG')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('NDCG')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e3ba1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# 추천여행지 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce634fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:43:13.128809Z",
     "start_time": "2024-04-08T18:43:13.128809Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 가중치 저장\n",
    "torch.save(lightGCN.state_dict(), '../model/LightGCN_best_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc112b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T23:23:54.765376Z",
     "start_time": "2024-04-08T23:23:54.543848Z"
    }
   },
   "outputs": [],
   "source": [
    "# 관광 메타 데이터 \n",
    "df = pd.read_csv('../data/travel_meta/travel_meta.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ebaec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T00:21:31.645704Z",
     "start_time": "2024-04-09T00:21:30.697406Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 사용자 ID 입력받기\n",
    "user_id = input(\"사용자 ID를 입력하세요: \")\n",
    "user_idx = user_label_encoder.transform([user_id])[0]\n",
    "\n",
    "lightGCN.eval()\n",
    "with torch.no_grad():\n",
    "    # 사용자 임베딩 가져오기\n",
    "    final_user_emb, final_item_emb, initial_user_emb, initial_item_emb = lightGCN.propagate_through_layers()\n",
    "    user_embedding = final_user_emb[user_idx]\n",
    "\n",
    "    # 아이템별 유사도 점수 계산\n",
    "    similarity_scores = torch.matmul(user_embedding, final_item_emb.transpose(0, 1))\n",
    "    recommended_item_indices = torch.topk(similarity_scores, top_k).indices\n",
    "\n",
    "# 추천된 아이템의 상세 정보를 포함한 데이터 프레임 생성\n",
    "recommended_items = []\n",
    "\n",
    "# 추천된 아이템들의 상세 정보 추출\n",
    "for item_idx in recommended_item_indices:\n",
    "    item_id = item_label_encoder.inverse_transform([item_idx.item()])[0]\n",
    "    item_info = df[df['POI_ID'] == item_id][['POI_ID', 'POI_NM', 'POI_TYPE']].iloc[0]\n",
    "    # item_info에 'REVISIT_INTENTION', 'RCMDTN_INTENTION', 'STARS' 정보 추가\n",
    "    item_info['REVISIT_INTENTION'] = data[data['item_id'] == item_id]['REVISIT_INTENTION'].iloc[0]\n",
    "    item_info['RCMDTN_INTENTION'] = data[data['item_id'] == item_id]['RCMDTN_INTENTION'].iloc[0]\n",
    "    item_info['STARS'] = data[data['item_id'] == item_id]['STARS'].iloc[0]\n",
    "    recommended_items.append(item_info)\n",
    "\n",
    "# 리스트를 데이터프레임으로 변환\n",
    "recommended_items_df = pd.concat(recommended_items, axis=1).transpose()\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"사용자 {user_id}에 대한 Top-{top_k} 추천 아이템:\")\n",
    "display(recommended_items_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405dd4e",
   "metadata": {},
   "source": [
    "# FastAPI 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc809d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class UserIDRequest(BaseModel):\n",
    "    user_id: str\n",
    "\n",
    "@app.post(\"/recommend/\")\n",
    "async def recommend(request: UserIDRequest):\n",
    "    user_id = request.user_id\n",
    "    user_idx = user_label_encoder.transform([user_id])[0]\n",
    "\n",
    "    lightGCN.eval()\n",
    "    with torch.no_grad():\n",
    "        # 사용자 임베딩 가져오기\n",
    "        final_user_emb, final_item_emb, initial_user_emb, initial_item_emb = lightGCN.propagate_through_layers()\n",
    "        user_embedding = final_user_emb[user_idx]\n",
    "\n",
    "        # 아이템별 유사도 점수 계산\n",
    "        similarity_scores = torch.matmul(user_embedding, final_item_emb.transpose(0, 1))\n",
    "        recommended_item_indices = torch.topk(similarity_scores, 5).indices  # top_k를 5로 가정\n",
    "\n",
    "    recommended_items = []\n",
    "\n",
    "    for item_idx in recommended_item_indices:\n",
    "        item_id = item_label_encoder.inverse_transform([item_idx.item()])[0]\n",
    "        item_info = df[df['POI_ID'] == item_id][['POI_ID', 'POI_NM', 'POI_TYPE']].iloc[0].to_dict()\n",
    "        item_info['REVISIT_INTENTION'] = data[data['item_id'] == item_id]['REVISIT_INTENTION'].iloc[0]\n",
    "        item_info['RCMDTN_INTENTION'] = data[data['item_id'] == item_id]['RCMDTN_INTENTION'].iloc[0]\n",
    "        item_info['STARS'] = data[data['item_id'] == item_id]['STARS'].iloc[0]\n",
    "        recommended_items.append(item_info)\n",
    "\n",
    "    return {\"user_id\": user_id, \"recommendations\": recommended_items}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "notify_time": "10",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.442px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
